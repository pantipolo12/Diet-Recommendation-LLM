{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pantipolo12/Diet-Recommendation-LLM/blob/main/IR_PROJECT_Diet_Recommendation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OCR (Optical Character Recognition)"
      ],
      "metadata": {
        "id": "YsJj-odvNP4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# Given Dataset (Chose relevant prescriptions)\n",
        "!gdown 1DvcXqFzEDlrFQTOQV_qEZjDWBT1MnnFl\n",
        "!gdown 1ENdiOHRYL8ImpFuBR9nLSRohF8nBmvJS\n",
        "!gdown 1rHIa19GiUb5xlG6ptdJXyaTs5GD8tyl0\n",
        "!gdown 196Ww4pmzDvRHNIhBU4zhwfO9cY7qQJI0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPc5e-VB3Be7",
        "outputId": "24832144-5910-4094-e058-bb43a18267db"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1DvcXqFzEDlrFQTOQV_qEZjDWBT1MnnFl\n",
            "To: /content/prescription 5.pdf\n",
            "100% 143k/143k [00:00<00:00, 12.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ENdiOHRYL8ImpFuBR9nLSRohF8nBmvJS\n",
            "To: /content/prescription 6.pdf\n",
            "100% 296k/296k [00:00<00:00, 5.30MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1rHIa19GiUb5xlG6ptdJXyaTs5GD8tyl0\n",
            "To: /content/prescription 7.pdf\n",
            "100% 255k/255k [00:00<00:00, 7.62MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=196Ww4pmzDvRHNIhBU4zhwfO9cY7qQJI0\n",
            "To: /content/prescription 8.pdf\n",
            "100% 338k/338k [00:00<00:00, 4.55MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pERmOcNmr71w"
      },
      "outputs": [],
      "source": [
        "# Installing OCR libraries\n",
        "%%capture\n",
        "!pip install pypdfium2\n",
        "!apt-get install poppler-utils\n",
        "!pip install pdf2image\n",
        "!apt install tesseract-ocr\n",
        "!apt install libtesseract-dev\n",
        "!pip install pytesseract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import pypdfium2 as pyp\n",
        "from pdf2image import convert_from_path\n",
        "from IPython.display import display, Image\n",
        "import cv2\n",
        "import pytesseract\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "tAcE527ksV92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1d0020d-ef82-4b25-d5da-a1a7befe1bda"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removes any skew in the image\n",
        "def deskew(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    gray = cv2.bitwise_not(gray)\n",
        "    coords = np.column_stack(np.where(gray > 0))\n",
        "    angle = cv2.minAreaRect(coords)[-1]\n",
        "\n",
        "    if angle < -45:\n",
        "        angle = -(90 + angle)\n",
        "    else:\n",
        "        angle = -angle\n",
        "\n",
        "    (h, w) = image.shape[:2]\n",
        "    center = (w // 2, h // 2)\n",
        "    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
        "    rotated = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
        "\n",
        "    return rotated\n",
        "\n",
        "# Extracts text from image\n",
        "def extract_text_from_image(image):\n",
        "    text = pytesseract.image_to_string(image)\n",
        "    return text"
      ],
      "metadata": {
        "id": "XBV6aemu4Tel"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the pdfs are stored\n",
        "data_dir = '/content'"
      ],
      "metadata": {
        "id": "QJvchPOO4UB4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns tokenized text extracted from pdf or images after new line/empty string removal\n",
        "def read_prescription(path):\n",
        "  pdf = pyp.PdfDocument(path)\n",
        "  pages = len(pdf)\n",
        "  text = []\n",
        "\n",
        "  for i in range(pages):\n",
        "      raw = pdf[i].get_textpage().get_text_range()\n",
        "      words = word_tokenize(raw)\n",
        "      text.extend(words)\n",
        "\n",
        "  # If the pdf is not read, that means we have an image and we read via OCR\n",
        "  if len(text) == 0:\n",
        "    pages = convert_from_path(path)\n",
        "    extracted_text = []\n",
        "\n",
        "    for page in pages:\n",
        "        preprocessed_image = deskew(np.array(page))\n",
        "        text1 = extract_text_from_image(preprocessed_image)\n",
        "        extracted_text.append(text1)\n",
        "\n",
        "    # The text extracted is erronous, we combine all of it and then split\n",
        "    # via spaces and remove unnecessary new line characters and empty strings\n",
        "    text.extend(extracted_text)\n",
        "    temp = \"\"\n",
        "    for x in text:\n",
        "      temp += x + \" \"\n",
        "    text = [temp]\n",
        "\n",
        "    text = text[0].split(\" \")\n",
        "    remove = []\n",
        "    for i in range(len(text)):\n",
        "      text[i] = re.sub('\\n', '', text[i])\n",
        "      if text[i] == \"\":\n",
        "        remove.append(i)\n",
        "    for i in remove[::-1]:\n",
        "      text.pop(i)\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "NCnVZPBX4dqr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find all the pdf files in the current directory\n",
        "pdf_files = [f for f in os.listdir(data_dir) if f.endswith('.pdf')]\n",
        "\n",
        "# Empty df\n",
        "df = pd.DataFrame(columns=['file', 'text'])\n",
        "title = []\n",
        "texts = []\n",
        "for pdf_file in pdf_files:\n",
        "  path = os.path.join(data_dir, pdf_file)\n",
        "  text = read_prescription(path)\n",
        "  texts.append(text)\n",
        "  title.append(pdf_file)\n",
        "\n",
        "df['file'] = title\n",
        "df['text'] = texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "id": "3qCtxCzt4Vs9",
        "outputId": "5185d195-6576-4414-f54f-469a4b4ac141"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2788897957.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpdf_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpdf_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpdf_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_prescription\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mtitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3740931005.py\u001b[0m in \u001b[0;36mread_prescription\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_textpage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m       \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m       \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting medications or advice\n",
        "keywords = [\"medication\", \"medicine\", \"advice\", \"advised\" , \"food\", \"diet\", \"meal\"]\n",
        "medicines = []\n",
        "for i in range(df.shape[0]):\n",
        "  prescription = df.iloc[i][1]\n",
        "  flag = False\n",
        "  for word in keywords:\n",
        "    for med in range(len(prescription)):\n",
        "      if word in prescription[med].lower():\n",
        "        prescription = prescription[med:]\n",
        "        flag = True\n",
        "      if flag:\n",
        "        break\n",
        "    if flag:\n",
        "      break\n",
        "  medicines.append(\" \".join(prescription))\n",
        "\n",
        "df[\"extracted\"] = medicines"
      ],
      "metadata": {
        "id": "kOSeQ80JEKAt",
        "outputId": "4228ef88-9ab7-40c5-95ab-41c3a899fd9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1641984314.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mkeywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"medication\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"medicine\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"advice\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"advised\"\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"food\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"diet\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"meal\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmedicines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mprescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "complete = []\n",
        "for i in range(df.shape[0]):\n",
        "  prescription = df.iloc[i][1]\n",
        "  prescription = \" \".join(prescription)\n",
        "  complete.append(prescription)\n",
        "\n",
        "df[\"complete\"] = complete"
      ],
      "metadata": {
        "id": "Kdy3cSp9Y0DI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "7eVmHJazZF5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[1][2]"
      ],
      "metadata": {
        "id": "KU5ek2Z4Eui5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gemini (A Generative AI Model developed by Google)"
      ],
      "metadata": {
        "id": "9FZMb6bQNNLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install google-generativeai"
      ],
      "metadata": {
        "id": "m49jvNuxJ2XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gemini API: {YOUR_API_KEY_HERE}\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"GEMINI_API_KEY\"] = \"{YOUR_API_KEY_HERE}\"\n",
        "gemini_key = os.environ[\"GEMINI_API_KEY\"]\n",
        "\n",
        "# Configure google-generativeai library with the API key\n",
        "genai.configure(api_key = gemini_key)"
      ],
      "metadata": {
        "id": "z_5_MaN1JilU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Supported models for generateContent method\n",
        "for m in genai.list_models():\n",
        "    if 'generateContent' in m.supported_generation_methods:\n",
        "        print(m.name)"
      ],
      "metadata": {
        "id": "1WXN0osZKMad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple Test for Gemini\n",
        "from IPython.display import Markdown\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "\n",
        "response = model.generate_content(\"Can you provide medical details of say DOLO tablets.\")\n",
        "\n",
        "Markdown(response.text) #display text as Markdown"
      ],
      "metadata": {
        "id": "3cjVaaMDKVg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prescription Test for Gemini\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "\n",
        "for i in range(df.shape[0]):\n",
        "\n",
        "  print(f\"Below Prescription: {df.iloc[i][0]}\")\n",
        "\n",
        "  response = model.generate_content(df.iloc[i][2] + \". Please identify medicines and their associated symptoms from this prescription, also identify associated diseases. Based on this give Food Recommendations.\")\n",
        "\n",
        "  display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "IwaoGS-VLIP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prescription Test for Gemini\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "\n",
        "for i in range(df.shape[0]):\n",
        "\n",
        "  print(f\"Below Prescription: {df.iloc[i][0]}\")\n",
        "\n",
        "  response = model.generate_content(df.iloc[i][3] + \". Please identify medicines and their associated symptoms from this prescription, also identify associated diseases.  Based on this give Food Recommendations. Structure response in simple table format\")\n",
        "\n",
        "  display(Markdown(response.text))"
      ],
      "metadata": {
        "id": "NdMfXdr6ZK--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.candidates[0].content.parts[0].text"
      ],
      "metadata": {
        "id": "fNLhIDPXM6RE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gemini_prompt(prompt):\n",
        "  import os\n",
        "  import google.generativeai as genai\n",
        "  from google.colab import userdata\n",
        "\n",
        "  os.environ[\"GEMINI_API_KEY\"] = \"{YOUR_API_KEY_HERE}\"\n",
        "  gemini_key = os.environ[\"GEMINI_API_KEY\"]\n",
        "\n",
        "  genai.configure(api_key = gemini_key)\n",
        "  model = genai.GenerativeModel('gemini-pro')\n",
        "  response = model.generate_content(prompt)\n",
        "  return response"
      ],
      "metadata": {
        "id": "uHyKUyJ_O6m3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llama (Large Language Model Meta AI)"
      ],
      "metadata": {
        "id": "avpPGZk4WMid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install llamaapi -q"
      ],
      "metadata": {
        "id": "o8Ts5I9dWN1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llamaapi import LlamaAPI\n",
        "import json\n",
        "\n",
        "# Replace 'Your_API_Token' with your actual API token\n",
        "llama = LlamaAPI('{YOUR_API_KEY_HERE}')"
      ],
      "metadata": {
        "id": "lwzuHxKAWQAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "# JSON object representing the API request\n",
        "api_request_json = {\n",
        "  \"model\": \"llama3-70b\", # model getting used\n",
        "  \"messages\": [\n",
        "    {\"role\": \"system\", \"content\": \"You are a llama assistant that talks like a llama, starting every word with 'll'.\"}, #system message\n",
        "    {\"role\": \"user\", \"content\": \"Hi, happy llama day!\"}, #user message\n",
        "  ]\n",
        "}\n",
        "\n",
        "# Make your request and handle the response\n",
        "response = llama.run(api_request_json)\n",
        "print(json.dumps(response.json(), indent=2))"
      ],
      "metadata": {
        "id": "Q3KesoLWWUK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prescription test\n",
        "api_request_json = {\n",
        "  \"model\": \"llama3-70b\",\n",
        "  \"messages\": [\n",
        "    {\"role\": \"system\", \"content\": \"Please provide information about different medicines, their symptoms and their associated diseases from the prescription.\"},\n",
        "    {\"role\": \"user\", \"content\": df.iloc[0][2]},\n",
        "    {\"role\": \"user\", \"content\": df.iloc[1][2]},\n",
        "    {\"role\": \"user\", \"content\": df.iloc[2][2]},\n",
        "    {\"role\": \"user\", \"content\": df.iloc[3][2]}\n",
        "  ]\n",
        "}\n",
        "\n",
        "# Make your request and handle the response\n",
        "response = llama.run(api_request_json)"
      ],
      "metadata": {
        "id": "sFPYW85MWdnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "Markdown(response)"
      ],
      "metadata": {
        "id": "k4OWmcuVXwZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Complete Pipeline"
      ],
      "metadata": {
        "id": "g_mBCv_fPLiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_diet_recommendation():\n",
        "  from google.colab import files\n",
        "  from IPython.display import Markdown, display\n",
        "  import time\n",
        "  # Ask the user to upload a file\n",
        "  print(\"Please upload a PDF file:\")\n",
        "  uploaded_file = files.upload()\n",
        "\n",
        "  # Check if a file was uploaded\n",
        "  if len(uploaded_file) > 0:\n",
        "      file_name = next(iter(uploaded_file))\n",
        "      if file_name.endswith(\".pdf\"):\n",
        "          print(\"File uploaded successfully:\", file_name)\n",
        "      else:\n",
        "          print(\"The uploaded file is not a PDF.\")\n",
        "  else:\n",
        "      print(\"No file uploaded.\")\n",
        "\n",
        "  print(\"Reading Text...\")\n",
        "  ocr = read_prescription(\"/content/\" + file_name)\n",
        "  ocr = \" \".join(ocr)\n",
        "  print(\"Identifying Diseases...\")\n",
        "  response = gemini_prompt(ocr + \". Identify medicines in this prescription, then find which disease is associated with them. The give food recommendations to help with the diseases. Give associated disease and food recommendation in tabular format\")\n",
        "  print(\"Generating Diet Recommendations...\")\n",
        "  display(Markdown(response.text))\n",
        "  return"
      ],
      "metadata": {
        "id": "CznlhDgcPK5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_diet_recommendation()"
      ],
      "metadata": {
        "id": "pL_tx2aoQdSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Did not Work"
      ],
      "metadata": {
        "id": "yRLBRFNbNFJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"%%capture\n",
        "!pip install transformers bitsandbytes\n",
        "!pip install git+https://github.com/huggingface/accelerate\"\"\""
      ],
      "metadata": {
        "id": "KJfnV0hVtMrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"from transformers import AutoTokenizer, LlamaTokenizer, LlamaForCausalLM\n",
        "import torch\n",
        "\n",
        "model_id = \"chaoyi-wu/PMC_LLAMA_7B\"\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "\n",
        "model = LlamaForCausalLM.from_pretrained(model_id, pad_token_id=tokenizer.eos_token_id, device_map=\"auto\", load_in_4bit=True)\"\"\""
      ],
      "metadata": {
        "id": "3dilhTUIqVHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"import transformers\n",
        "import torch\n",
        "tokenizer = transformers.LlamaTokenizer.from_pretrained('chaoyi-wu/PMC_LLAMA_7B')\n",
        "model = transformers.LlamaForCausalLM.from_pretrained('chaoyi-wu/PMC_LLAMA_7B', pad_token_id=tokenizer.eos_token_id)\n",
        "model.cuda()  # move the model to GPU\"\"\""
      ],
      "metadata": {
        "id": "Ol2l1jjtn2JH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xu4WYZgQNDKN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}